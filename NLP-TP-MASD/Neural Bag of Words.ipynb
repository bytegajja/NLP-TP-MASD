{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36e41e8",
   "metadata": {},
   "source": [
    "# 1 - NBoW\n",
    "\n",
    "In this series we'll be building a machine learning model to perform sentiment analysis -- a subset of text classification where the task is to detect if a given sentence is positive or negative -- using [PyTorch](https://github.com/pytorch/pytorch) and [torchtext](https://github.com/pytorch/text). The dataset used will be movie reviews from the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which we'll obtain using the [datasets](https://github.com/huggingface/datasets) library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b771bb5",
   "metadata": {},
   "source": [
    "# imdb\n",
    "\n",
    "To load the IMDB movie review dataset, we use the load_dataset function from the Hugging Face Datasets library. This code splits the dataset into two parts: a training set and a test set. The training set (stored in the variable train_data) is used to train the text classification model, while the test set (stored in test_data) is used to evaluate the model’s performance. The IMDB dataset is a popular choice for sentiment analysis tasks, as it contains movie reviews labeled as positive or negative, providing an ideal framework for developing and testing classification models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdce176",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this first notebook, we'll start very simple with one of the most basic models for _NLP_ (natural language processing): a _NBoW_ (_neural bag-of-words_) model (also known as _continuous bag-of-words_, _CBoW_). The NBoW model are a strong, commonly used, baseline model for NLP tasks. They should be one of the first models you implement when performing sentiment analysis/text classification.\n",
    "\n",
    "![](assets/nbow_model.png)\n",
    "\n",
    "An NBoW model takes in a sequence of $T$ _tokens_, $X=\\{x_1,...,x_T\\} \\in \\mathbb{Z}^T$ and passes each token through an _embedding layer_ to obtain a sequence of _embedding vectors_. The sequence of embedding vectors is just known as an _embedding_, $E=\\{e_1,...,e_T\\} \\in \\mathbb{R}^{T \\times D}$, where $D$ is known as the _embedding dimension_. It then _pools_ the embeddings across the sequence dimension to get $P \\in \\mathbb{R}^D$ and then finally passes $P$ through a linear layer (also known as a fully connected layer), to get a prediction, $\\hat{Y} \\in \\mathbb{R}^C$, where $C$ is the number of classes. We'll explain what a token is, and what each of the layers -- embedding layer, pooling, and linear layer -- do in due course.\n",
    "\n",
    "A note on notation, what does something like $E=\\{e_1,...,e_T\\} \\in \\mathbb{R}^{T \\times D}$ mean? $\\mathbb{R}^{T \\times D}$ means a $T \\times D$ sized tensor full of real numbers, i.e. a `torch.FloatTensor`. $X=\\{x_1,...,x_T\\} \\in \\mathbb{Z}^T$ is a $T$ sized tensor full of integers, i.e. a `torch.LongTensor`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ad599",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "Before we can implement our NBoW model, we first have to perform quite a few steps to get our data ready to use. NLP usually requires quite a lot of data wrangling beforehand, though libraries such as `datasets` and `torchtext` handle most of this for us.\n",
    "\n",
    "The steps to take are:\n",
    "\n",
    "-   importing modules\n",
    "-   loading data\n",
    "-   tokenizing data\n",
    "-   creating data splits\n",
    "-   creating a vocabulary\n",
    "-   numericalizing data\n",
    "-   creating the data loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c264adc",
   "metadata": {},
   "source": [
    "## 1. Importing Modules\n",
    "In this step, we import all the necessary libraries and modules required to work with the dataset, perform tokenization, and build the model. These include libraries for data manipulation, model building, and utility functions.\n",
    "## 2. Loading Data\n",
    "Here, we load the IMDB dataset using the Hugging Face load_dataset function. The dataset is divided into two parts: the training set and the test set, which are used for training and evaluation of the model, respectively.\n",
    "## 3. Tokenizing Data\n",
    "In this step, we convert the raw text into tokens that the model can understand. Tokenization is a crucial part of natural language processing, as it transforms words or sentences into numerical representations.\n",
    "## 4. Creating Data Splits\n",
    "If you need to split your data further (for example, for validation purposes), you can divide the training set into smaller parts. Here, we assume the training data is already split into training and testing sets, but additional splits could be done if required.\n",
    "## 5. Creating a Vocabulary\n",
    "A vocabulary is built based on the tokens found in the dataset. This allows the model to learn a mapping between tokens and their corresponding indices.\n",
    "## 6. Numericalizing Data\n",
    "Numericalizing is the process of converting tokens into numerical representations (usually indices) that can be fed into a machine learning model. This step uses the tokenizer to convert the text into sequences of token indices.\n",
    "## 7. Creating the Data Loaders\n",
    "Data loaders are used to efficiently load batches of data during training and evaluation. They allow the model to process the data in manageable chunks, reducing memory usage and speeding up training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e085b",
   "metadata": {},
   "source": [
    "### 1. Tokenization and Embedding\n",
    "In Natural Language Processing (NLP), the first step is to convert raw text into a format that a machine learning model can process. This involves **tokenization**, which splits the text into smaller units (tokens), and **embedding**, which transforms each token into a vector of numbers. Models like BERT use **pre-trained embeddings** to perform this transformation.\n",
    "\n",
    "- **Tokenization**: This step breaks down the text into tokens, which can be words or sub-words (depending on the tokenizer). Each token is then mapped to a numerical value.\n",
    "  \n",
    "- **Embedding**: After tokenization, the tokens pass through an embedding layer that transforms each token into a dense vector representation. These vectors capture the semantic meaning of the tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Attention Mechanism (Self-Attention)\n",
    "A core feature of transformer models like BERT is the **self-attention mechanism**, which enables the model to understand the relationships between tokens in a sentence. This helps the model capture contextual meaning more effectively than older models such as Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs).\n",
    "\n",
    "For each token, the model generates three vectors:\n",
    "- **Query (Q)**\n",
    "- **Key (K)**\n",
    "- **Value (V)**\n",
    "\n",
    "The self-attention mechanism calculates how each token (Query) relates to every other token (Key) in the sequence. This results in an **attention score** that determines how much attention each token should give to others in the sequence.\n",
    "\n",
    "The formula for the attention mechanism is:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\cdot V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( Q \\) is the query matrix (from the token representation)\n",
    "- \\( K \\) is the key matrix (from the token representation)\n",
    "- \\( V \\) is the value matrix (from the token representation)\n",
    "- \\( d_k \\) is the dimension of the key (used for scaling)\n",
    "\n",
    "The attention score is computed by taking the dot product of the query and key matrices, scaling it by the square root of the key dimension, and applying a softmax function. This results in a weighted sum of the value vectors.\n",
    "\n",
    "This process is repeated through multiple layers, often using **multi-head attention**, which captures different aspects of relationships between tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Positional Encoding\n",
    "Since transformers don’t inherently understand the order of tokens (unlike RNNs), **positional encodings** are added to the token embeddings. These encodings provide the model with information about the position of tokens in the sequence.\n",
    "\n",
    "The positional encoding is often computed using sine and cosine functions:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}(i, 2k) = \\sin\\left(\\frac{i}{10000^{2k/d}}\\right), \\quad \\mathbf{p}(i, 2k+1) = \\cos\\left(\\frac{i}{10000^{2k/d}}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( i \\) is the position of the token in the sequence\n",
    "- \\( k \\) is the dimension of the positional encoding\n",
    "- \\( d \\) is the dimension of the token embedding\n",
    "\n",
    "These encodings are added to the token embeddings to inform the model about the relative positions of tokens in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Feedforward Neural Networks\n",
    "After applying the attention layers, the model passes the resulting token representations through a **feedforward neural network** (FFNN). This network usually consists of two fully connected layers with non-linear activation functions (such as ReLU or GELU).\n",
    "\n",
    "Mathematically, for an input vector \\( \\mathbf{z} \\) from the attention mechanism, the feedforward pass is:\n",
    "\n",
    "$$\n",
    "\\mathbf{z'} = \\text{ReLU}(\\mathbf{W_1} \\cdot \\mathbf{z} + \\mathbf{b_1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{z''} = \\mathbf{W_2} \\cdot \\mathbf{z'} + \\mathbf{b_2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{W_1} \\) and \\( \\mathbf{W_2} \\) are the weight matrices\n",
    "- \\( \\mathbf{b_1} \\) and \\( \\mathbf{b_2} \\) are the bias vectors\n",
    "- The ReLU function introduces non-linearity to allow the model to learn complex patterns\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Output Layer\n",
    "After passing through the attention and feedforward layers, the model produces a final representation for each input sequence. In classification tasks like sentiment analysis, this output corresponds to a classification score for each class (e.g., positive or negative sentiment).\n",
    "\n",
    "The output layer typically involves a linear transformation followed by a softmax function, which converts the raw scores into probabilities:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\text{softmax}(\\mathbf{W_{out}} \\cdot \\mathbf{z_{final}} + \\mathbf{b_{out}})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{W_{out}} \\) is the output weight matrix\n",
    "- \\( \\mathbf{z_{final}} \\) is the final token embedding after passing through the layers\n",
    "- \\( \\mathbf{b_{out}} \\) is the output bias vector\n",
    "\n",
    "The softmax function normalizes the output into a probability distribution over the possible classes.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Loss Function\n",
    "The model is trained to minimize the difference between its predicted probabilities and the actual labels using a loss function. For classification tasks, **cross-entropy loss** is commonly used:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{i=1}^{N} y_i \\cdot \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( N \\) is the number of classes\n",
    "- \\( y_i \\) is the true label for class \\( i \\) (1 for the correct class, 0 for others)\n",
    "- \\( \\hat{y}_i \\) is the predicted probability for class \\( i \\)\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Optimization\n",
    "The model is trained using **gradient-based optimization techniques** like **Adam** or **Stochastic Gradient Descent (SGD)**. The parameters (weights) of the model are updated to minimize the loss function.\n",
    "\n",
    "The update rule for a parameter \\( \\theta \\) is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $$ \\eta $$ is the learning rate\n",
    "- $$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} $$ is the gradient of the loss function with respect to the parameter $$ \\theta $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c4cf4",
   "metadata": {},
   "source": [
    "### Importing Modules\n",
    "\n",
    "First, we'll import the required modules.\n",
    "\n",
    "We use the `datasets` module for handling datasets, `matplotlib` for plotting our results, `numpy` for numerical processing, `torch` for tensor computations, `torch.nn` for neural networks, `torch.optim` for neural network optimizers, `torchtext` for text processing, and `tqdm` for measuring progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c67fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36da91ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchtext==0.15.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch==2.0.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch==2.0.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch==2.0.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch==2.0.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch==2.0.1) (3.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchtext==0.15.2) (4.67.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchtext==0.15.2) (2.32.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchtext==0.15.2) (1.24.3)\n",
      "Requirement already satisfied: torchdata==0.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchtext==0.15.2) (0.6.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchdata==0.6.1->torchtext==0.15.2) (1.26.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.1) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchtext==0.15.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchtext==0.15.2) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchtext==0.15.2) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->torchtext==0.15.2) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.1 torchtext==0.15.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15bf3a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)               # Version de PyTorch\n",
    "print(torch.cuda.is_available())       # Vérifie la disponibilité de la GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e322bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5478fc3",
   "metadata": {},
   "source": [
    "We'll also make sure to set the random seeds for `torch` and `numpy`. This is to ensure this notebook is reproducable, i.e. we get the same results each time we run it.\n",
    "\n",
    "It is usually good practice to run your experiments multiple times with different random seeds -- both to measure the variance of your model and also to avoid having results only calculated with either \"good\" or \"bad\" seeds, i.e. being very lucky or unlucky with the randomness in the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc98ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1eb74",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Next, we'll load our dataset using the `datasets` library. The first argument is the name of the dataset and the `split` argument chooses which _splits_ of the data we want.\n",
    "\n",
    "Datasets usually come in two or more _splits_, non-overlapping examples from the data, most commonly a _train split_ -- which we train our model on -- and a _test split_ -- which we evaluate our trained model on. There's also a _validation split_, which we'll talk more about later. The train, test and validation split are also commonly called the train, test and validation sets -- we'll use split and set interchangeably\n",
    "in these tutorials -- and the dataset usually refers to all three of the sets combined. The IMDb dataset actually comes with a third split, called _unsupervised_, which contains a bunch of examples without labels. We don't want these so we don't include them in our `split` argument. Note that if we didn't pass an argument to `split` then it would load all available splits of the data.\n",
    "\n",
    "How do we know that we have to use \"imdb\" for the IMDb dataset and that there's an \"unsupervised\" split? The `datasets` library has a great website used to browse the available datasets, see: https://huggingface.co/datasets/. By navigating to the [IMDb dataset page](https://huggingface.co/datasets/imdb) we can see more information specifically about the IMDb dataset.\n",
    "\n",
    "The output received when loading the dataset tells us that it is using a locally cached version instead of downloading the dataset from online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798f5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93721296",
   "metadata": {},
   "source": [
    "We can print out the splits which shows us the _features_ and _num_rows_ of the dataset. num*rows are the number of examples in split, as we can see, there are 25,000 examples in each. Each example in a dataset provided by the `datasets` library is a dictionary, and the features are the keys which appear in every one of those dictionaries/examples. So, each example in the IMDb dataset has a \\_text* and a _label_ key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42338609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 25000\n",
       " }))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec70556",
   "metadata": {},
   "source": [
    "We can check the `features` attribute of a split to get more information about the features. We can see that _text_ is a `Value` of `dtype=string` -- in other words, it's a string -- and that _label_ is a `ClassLabel`. A `ClassLabel` means the feature is an integer representation of which class the example belongs to. `num_classes=2` means that our labels are one of two values, 0 or 1, and `names=['neg', 'pos']` gives us the human-readable versions of those values. Thus, a label of 0 means the example is a negative review and a label of 1 means the example is a positive review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58f5cc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84271369",
   "metadata": {},
   "source": [
    "We can look at an example by indexing into the train set. As we can see, the text is quite noisy and also rambles on quite a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a6e8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8536207",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "One of the first things we need to do to our data is _tokenize_ it. Machine learning models aren't designed to handle strings, they're design to handle numbers. So what we need to do is break down our string into individual _tokens_, and then convert these tokens to numbers. We'll get to the conversion later, but first we'll look at _tokenization_.\n",
    "\n",
    "Tokenization involves using a _tokenizer_ to process the strings in our dataset. A tokenizer is a function that goes from a string to a list of strings. There are many types of tokenizers available, but we're going to use a relatively simple one provided by `torchtext` called the `basic_english` tokenizer. We load our tokenizer as such:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3017c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db58859",
   "metadata": {},
   "source": [
    "We can use the tokenizer by calling it on a string.\n",
    "\n",
    "Notice it creates a token by splitting the word on spaces, separating punctuation into its own token, and also lowercasing every word.\n",
    "\n",
    "The `get_tokenizer` function also supports other tokenizers, such as ones provided by [spaCy](https://spacy.io/) and [nltk](https://www.nltk.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d0de969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'world',\n",
       " '!',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'doing',\n",
       " 'fantastic',\n",
       " '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world! How are you doing today? I'm doing fantastic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593711b9",
   "metadata": {},
   "source": [
    "Now we have our tokenizer defined, we want to actually tokenize our data.\n",
    "\n",
    "Each dataset provided by the `datasets` library is an instance of a `Dataset` class. We can see all the methods in a `Dataset` [here](https://huggingface.co/docs/datasets/package_reference/main_classes.html#dataset), but the main one we are interested in is [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map). By using `map` we can apply a function to every example in the dataset and either update the example or create a new feature.\n",
    "\n",
    "We define the `tokenize_example` function below which takes in an `example`, a `tokenizer` and a `max_length` argument, tokenizes the text in the example, given by `example['text']`, trims the tokens to a maximum length and then returns a dictionary with the new feature name and feature value for that example. Note that the first argument to a function which we are going to `map` must always be the example dictionary, and it must always return a dictionary where the keys are the feature names and the values are the feature values to be added to this example.\n",
    "\n",
    "We're trimming the tokens to a maximum length here as some examples are unnecessarily long and we can predict sentiment pretty well just using the first couple of hundred tokens -- though this might not be true for you if you're using a different dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876ad3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, tokenizer, max_length):\n",
    "    tokens = tokenizer(example[\"text\"])[:max_length]\n",
    "    return {\"tokens\": tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35129a1b",
   "metadata": {},
   "source": [
    "We apply the `tokenize_example` function below, on both the train and test sets. Any arguments to the function -- that aren't the example -- need to be passed as the `fn_kwargs` dictionary, with the keys being the argument names and the values the value passed to that argument.\n",
    "\n",
    "Operations on a `Dataset` are **not** performed in-place. You should always return the result into a new variable.\n",
    "\n",
    "Note the warnings showing that as I have performed this `map` before, the results are cached and are thus loaded from the cache instead of being calculated again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e295030",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "train_data = train_data.map(\n",
    "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "test_data = test_data.map(\n",
    "    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b38c0",
   "metadata": {},
   "source": [
    "We can now see that our `train_data` has a _tokens_ feature -- as \"tokens\" was a key in the dictionary returned by the function we used for the `map`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f647bdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'tokens'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3443a0",
   "metadata": {},
   "source": [
    "By looking at the `features` attribute we can see it has automatically added the information about the tokens feature -- each is a sequence (a list) of strings. A `length=-1` means that all of our token sequences are not the same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1605d52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1735d91a",
   "metadata": {},
   "source": [
    "We can check the first example in our train set to see the result of the tokenization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f3de3b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'rented',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious-yellow',\n",
       " 'from',\n",
       " 'my',\n",
       " 'video',\n",
       " 'store',\n",
       " 'because',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'controversy',\n",
       " 'that',\n",
       " 'surrounded',\n",
       " 'it',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'first',\n",
       " 'released',\n",
       " 'in',\n",
       " '1967',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"tokens\"][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4ee14",
   "metadata": {},
   "source": [
    "### Creating Validation Data\n",
    "\n",
    "Next up, we'll create a _validation set_ from our data. This is similar to our test set in that we do not train our model on it, we only evaluate our model on it.\n",
    "\n",
    "Why have both a validation set and a test set? Your test set respresents the real world data that you'd see if you actually deployed this model. You won't be able to see what data your model will be fed once deployed, and your test set is supposed to reflect that. Every time we tune our model hyperparameters or training set-up to make it do a bit better on the test set, we are leak information from the test set into the training process. If we do this too often then we begin to overfit on the test set. Hence, we need some data which can act as a \"proxy\" test set which we can look at more frequently in order to evaluate how well our model actually does on unseen data -- this is the validation set.\n",
    "\n",
    "We can split a `Dataset` using the `train_test_split` method which splits a dataset into two, creating a `DatasetDict` for each split, one called `train` and another called `test` -- a bit confusing because these are our train and validation sets, not the test. We use `test_size` to set the portion of the data used for the validation set -- 0.25 means we use 25% of the training set -- and the examples are chosen randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15e48bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.25\n",
    "\n",
    "train_valid_data = train_data.train_test_split(test_size=test_size)\n",
    "train_data = train_valid_data[\"train\"]\n",
    "valid_data = train_valid_data[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c829b",
   "metadata": {},
   "source": [
    "By showing the lengths of each split within our dataset, we can see the 25,000 training examples have now been split into 18,750 training examples and 6,250 validation examples, with the original 25,000 test examples remaining untouched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c227e4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18750, 6250, 25000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078d481",
   "metadata": {},
   "source": [
    "### Creating a Vocabulary\n",
    "\n",
    "Next, we have to build a _vocabulary_. This is look-up table where every unique token in your dataset has a corresponding _index_ (an integer).\n",
    "\n",
    "We do this as machine learning models cannot operate on strings, only numerical vaslues. Each _index_ is used to construct a _one-hot_ vector for each token. A one-hot vector is a vector where all the elements are 0, except one, which is 1, and the dimensionality is the total number of unique tokens in your vocabulary, commonly denoted by $V$.\n",
    "\n",
    "For example:\n",
    "\n",
    "![](assets/vocabulary.png)\n",
    "\n",
    "One issue with creating a vocabulary using every single word in the dataset is that there are usually a considerable amount of unique tokens. One way to combat this is to either only construct the vocabulary only using the most commonly appearing tokens, or to only use tokens which appear a minimum amount of times in the dataset. In this notebook, we do the latter, keeping on the tokens which appear 5 times.\n",
    "\n",
    "What happens to tokens which appear less than 5 times? We replace them with a special _unknown_ token, denoted by `<unk>`. For example, if the sentence \"This film is great and I love it\", but the word \"love\" was not in the vocabulary, it would become: \"This film is great and I \\<unk\\> it\".\n",
    "\n",
    "We use the `build_vocab_from_iterator` function from `torchtext.vocab` to create our vocabulary, specifying the `min_freq` (the minimum amount of times a token should appear to be added to the vocabulary) and `special_tokens` (tokens which should be appended to the start of the vocabulary, even if they don't appear `min_freq` times in the dataset).\n",
    "\n",
    "The first special token is our unknown token, the other, `<pad>` is a special token we'll use for padding sentences.\n",
    "\n",
    "When we feed sentences into our model, we pass a _batch_ of sentences, i.e. more than one, at the same time. Passing a batch of sentences is preferred to passing sentences one at a time as it allows our model to perform computation on all sentences within a batch in paralle, thus speeding up the time taken to train and evaluate our model. All sentences within a batch need to be the same length (in terms of the number of tokens). Thus, to ensure each sentence is the same length, any shorter than the longest sentence need to have padding tokens appended to the end of them.\n",
    "\n",
    "For an example batch of two sentences of length four and three tokens:\n",
    "\n",
    "![](assets/padding.png)\n",
    "\n",
    "As we can see, the second sentence has been padded with a single `<pad>` token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4865e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d546eb",
   "metadata": {},
   "source": [
    "Why is the vocabulary built from only the training data? When testing any machine learning system, we want to avoid any form of data leakage but not using any information from the test data -- this includes the frequency of tokens within the test data. As the validation set is supposed to reflect the test set as much as possible, we also do not use it to build the vocabulary. A common mistake is building the vocabulary using validation and test data -- do not do this!\n",
    "\n",
    "Now we have our vocabulary, we can first examine it by checking its length -- the number of tokens in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "123ceb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21635"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549dd0df",
   "metadata": {},
   "source": [
    "We can view the tokens in our vocabulary using the `get_itos` method, which returns a list of strings (tokens), and the index of each token in the list is the index of the token in our vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4ec89de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', '.', ',', 'a', 'and', 'of', 'to', \"'\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d83bc",
   "metadata": {},
   "source": [
    "We can get the index of a token by accessing it like a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3cabb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"and\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf15880",
   "metadata": {},
   "source": [
    "We store the indices of the unknown and padding tokens (zero and one, respectively) in variables, as we'll use these further on in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29ac49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f4677",
   "metadata": {},
   "source": [
    "We can check if a token is in our vocabulary using the `in` operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "201b5383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"some_token\" in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cac27f",
   "metadata": {},
   "source": [
    "By default, a vocabulary created by `torchtext` will throw an error if you attempt to obtain the index of a token which is not in the vocabulary, i.e. `vocab[\"some_token\"]` will throw an error.\n",
    "\n",
    "We need to explicity tell the vocabulary which token to return if we pass a token not in the vocabulary. We do this using the `set_default_index` method, passing in the index we wish it to return. Here, we pass the index of the unknown token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a951ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e90674",
   "metadata": {},
   "source": [
    "Now, when trying to get the index of a token that is not in the vocabulary, instead of throwing an error we get zero, the value of `unk_index`, our unknown token, `<unk>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "407fe05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"some_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a61f1e",
   "metadata": {},
   "source": [
    "To look-up a list of tokens, we can use the vocabulary's `lookup_indices` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbef1d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5516, 184, 0, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_indices([\"hello\", \"world\", \"some_token\", \"<pad>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31fcd7c",
   "metadata": {},
   "source": [
    "### Numericalizing Data\n",
    "\n",
    "Now we have our vocabulary, we can numericalize our data. This involves converting the tokens within our dataset into indices. Similar to how we tokenized our data using the `Dataset.map` method, we'll define a function that takes an example and our vocabulary, gets the index for each token in each example and then creates an `ids` field which containes the numericalized tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76518d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, vocab):\n",
    "    ids = vocab.lookup_indices(example[\"tokens\"])\n",
    "    return {\"ids\": ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbffcc",
   "metadata": {},
   "source": [
    "We then apply this function to all examples in the training, validation and testing datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dacaeaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1442c16",
   "metadata": {},
   "source": [
    "Checking an example, we can see that the `id` field now consists of the indexes of the tokens from that example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9ea2af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look', ',', 'this', 'is', 'quite', 'possibly', 'one', 'of', 'the', 'best']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"tokens\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaaa9421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[180, 4, 14, 10, 191, 841, 34, 7, 2, 121]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_indices(train_data[0][\"tokens\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "700f096d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[180, 4, 14, 10, 191, 841, 34, 7, 2, 121]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"ids\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95646b",
   "metadata": {},
   "source": [
    "The final step of numericalization is transforming the `ids` and `label` from integers into PyTorch tensors, which we do using the `with_format` method.\n",
    "\n",
    "We do this because our PyTorch models work with tensors, and not integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "678d0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.with_format(type=\"torch\", columns=[\"ids\", \"label\"])\n",
    "valid_data = valid_data.with_format(type=\"torch\", columns=[\"ids\", \"label\"])\n",
    "test_data = test_data.with_format(type=\"torch\", columns=[\"ids\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "221c3e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9d4ec10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([180,   4,  14,  10, 191, 841,  34,   7,   2, 121])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"ids\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829fd5f",
   "metadata": {},
   "source": [
    "One thing to note is that when using `with_format`, all the columns not specified (`\"tokens\"` and `\"text\"`) are removed from the example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90f7a79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'ids'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c16016",
   "metadata": {},
   "source": [
    "Removing the \"tokens\" field is fine, as if we wanted to retrieve the human-readable tokens again we can simply convert the tensor into a Python list of integers and then use the vocabulary's `lookup_tokens` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4228c9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look', ',', 'this', 'is', 'quite', 'possibly', 'one', 'of', 'the', 'best']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens(train_data[0][\"ids\"][:10].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56d82c",
   "metadata": {},
   "source": [
    "### Creating Data Loaders\n",
    "\n",
    "The final step of preparing the data is creating the data loaders. We can iterate over a data loader to retrieve batches of examples. This is also where we will perform any padding that is necessary.\n",
    "\n",
    "We first need to define a function to _collate_ a batch, consisting of a list of examples, into what we want our data loader to output.\n",
    "\n",
    "Here, our desired output from the data loader is a dictionary with keys of `\"ids\"` and `\"label\"`.\n",
    "\n",
    "The value of `batch[\"ids\"]` should be a tensor of shape `[batch size, length]`, where `length` is the length of the longest sentence (in terms of tokens) within the batch, and all sentences shorter than this should be padded to that length.\n",
    "\n",
    "The value of `batch[\"label\"]` should be a tensor of shape `[batch size]` consisting of the label for each sentence in the batch.\n",
    "\n",
    "We define a function, `get_collate_fn`, which is passed the pad token index and returns the actual collate function. Within the actual collate function, `collate_fn`, we get a list of `\"ids\"` tensors for each example in the batch, and then use the `pad_sequence` function, which converts the list of tensors into the desired `[batch size, length]` shaped tensor and performs padding using the specified `pad_index`. By default, `pad_sequence` will return a `[length, batch size]` shaped tensor, but by setting `batch_first=True`, these two dimensions are switched. We get a list of `\"label\"` tensors and convert the list of tensors into a single `[batch size]` shaped tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d39cf1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_ids = [i[\"ids\"] for i in batch]\n",
    "        batch_ids = nn.utils.rnn.pad_sequence(\n",
    "            batch_ids, padding_value=pad_index, batch_first=True\n",
    "        )\n",
    "        batch_label = [i[\"label\"] for i in batch]\n",
    "        batch_label = torch.stack(batch_label)\n",
    "        batch = {\"ids\": batch_ids, \"label\": batch_label}\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc449c7",
   "metadata": {},
   "source": [
    "Next, we define a function which returns our actual data loader. It takes in a dataset, desired batch size (the number of sentences we want in a batch), our padding token index, and if the dataset should be shuffled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c88f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677e78a",
   "metadata": {},
   "source": [
    "Finally, we get the data loaders for the training, validation and test data.\n",
    "\n",
    "We set the batch size equal to 512. Our batch size should be set as high as we can, as larger batches means more parallel computation, less compute time, and thus faster training and evaluation.\n",
    "\n",
    "Only the training data loader needs to be shuffled, as it's the only one used to actually tune the parameters within the model, and your training data should always be shuffled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3098a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6245d0",
   "metadata": {},
   "source": [
    "## Building the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "081f04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBoW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        # ids = [batch size, seq len]\n",
    "        embedded = self.embedding(ids)\n",
    "        # embedded = [batch size, seq len, embedding dim]\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        # pooled = [batch size, embedding dim]\n",
    "        prediction = self.fc(pooled)\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97897898",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "output_dim = len(train_data.unique(\"label\"))\n",
    "\n",
    "model = NBoW(vocab_size, embedding_dim, output_dim, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4acc5118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,491,102 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10e0b6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████▉| 399999/400000 [03:19<00:00, 2001.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Chargement des vecteurs GloVe depuis le cache local\n",
    "vectors = GloVe(name='6B', dim=300, cache=\"C:/Users/HP/Desktop/nlp classification puthor/pytorch-sentiment-analysis laste/.vector_cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ead7be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_vector = vectors.get_vecs_by_tokens(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a64ead7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ecc5d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3371, -0.2169, -0.0066, -0.4162, -1.2555, -0.0285, -0.7219, -0.5289,\n",
       "         0.0072,  0.3200,  0.0294, -0.0132,  0.4351,  0.2572,  0.3900, -0.1197,\n",
       "         0.1504,  0.4476,  0.2841,  0.4934,  0.6283,  0.2289, -0.4038,  0.0274,\n",
       "         0.0074,  0.1400,  0.2335,  0.0681,  0.4842, -0.0196, -0.5475, -0.5498])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hello_vector[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8540b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d31228e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21635, 300])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a6f4173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1117, -0.4966,  0.1631,  ..., -0.5592, -0.4480, -0.6476],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.7882, -1.6625, -0.7064,  ..., -1.5841, -0.3711, -1.2338],\n",
       "        ...,\n",
       "        [-0.1833,  0.2245, -0.3846,  ..., -0.8093, -1.5803,  2.2097],\n",
       "        [ 0.8763, -0.4503, -0.3297,  ...,  1.0524, -0.3182, -0.0381],\n",
       "        [-0.7596,  0.6286,  0.0057,  ..., -0.8484, -0.1224,  1.3181]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c1cbd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
       "        ...,\n",
       "        [ 0.3934, -0.6968, -0.0881,  ..., -0.0870,  0.6774, -0.2201],\n",
       "        [ 0.3478,  0.0483, -0.1087,  ..., -0.0443,  0.4599,  0.2786],\n",
       "        [-0.1583, -0.0640, -0.3428,  ..., -0.5484,  0.3303,  0.0076]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ea34c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1332d9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
       "        ...,\n",
       "        [ 0.3934, -0.6968, -0.0881,  ..., -0.0870,  0.6774, -0.2201],\n",
       "        [ 0.3478,  0.0483, -0.1087,  ..., -0.0443,  0.4599,  0.2786],\n",
       "        [-0.1583, -0.0640, -0.3428,  ..., -0.5484,  0.3303,  0.0076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4fcb95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f8829cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ed273e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3cdaf3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "729aa9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    for batch in tqdm.tqdm(data_loader, desc=\"training...\"):\n",
    "        ids = batch[\"ids\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        prediction = model(ids)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0a80c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(data_loader, desc=\"evaluating...\"):\n",
    "            ids = batch[\"ids\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "            prediction = model(ids)\n",
    "            loss = criterion(prediction, label)\n",
    "            accuracy = get_accuracy(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "703aa1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31343f1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      2\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m metrics \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m      7\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m      8\u001b[0m         train_data_loader, model, criterion, optimizer, device\n\u001b[0;32m      9\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "metrics = collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train(\n",
    "        train_data_loader, model, criterion, optimizer, device\n",
    "    )\n",
    "    valid_loss, valid_acc = evaluate(valid_data_loader, model, criterion, device)\n",
    "    metrics[\"train_losses\"].append(train_loss)\n",
    "    metrics[\"train_accs\"].append(train_acc)\n",
    "    metrics[\"valid_losses\"].append(valid_loss)\n",
    "    metrics[\"valid_accs\"].append(valid_acc)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"nbow.pt\")\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    print(f\"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}\")\n",
    "    print(f\"valid_loss: {valid_loss:.3f}, valid_acc: {valid_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d791c70",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      2\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(metrics[\"train_losses\"], label=\"train loss\")\n",
    "ax.plot(metrics[\"valid_losses\"], label=\"valid loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xticks(range(n_epochs))\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc422190",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(metrics[\"train_accs\"], label=\"train accuracy\")\n",
    "ax.plot(metrics[\"valid_accs\"], label=\"valid accuracy\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_xticks(range(n_epochs))\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac26e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"nbow.pt\"))\n",
    "\n",
    "test_loss, test_acc = evaluate(test_data_loader, model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d741147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test_loss: {test_loss:.3f}, test_acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, vocab, device):\n",
    "    tokens = tokenizer(text)\n",
    "    ids = vocab.lookup_indices(tokens)\n",
    "    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n",
    "    prediction = model(tensor).squeeze(dim=0)\n",
    "    probability = torch.softmax(prediction, dim=-1)\n",
    "    predicted_class = prediction.argmax(dim=-1).item()\n",
    "    predicted_probability = probability[predicted_class].item()\n",
    "    return predicted_class, predicted_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is terrible!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da60d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is great!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is not terrible, it's great!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d55c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This film is not great, it's terrible!\"\n",
    "\n",
    "predict_sentiment(text, model, tokenizer, vocab, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
